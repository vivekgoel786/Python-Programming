1. Use a Non-Linear Scaling (Power Transformation)

Instead of scaling linearly, you can use a power transformation to stretch the probability distribution:

power = 1.2  # Experiment with values between 1.1 and 2
adjusted_probs = np.power(calibrated_probabilities, power)

# Normalize to ensure the average remains at the target CT
scaled_calibrated_probabilities = (adjusted_probs / np.mean(adjusted_probs)) * target_ct_value

This transformation makes lower probabilities smaller and higher probabilities larger, increasing spread.


---

2. Use Min-Max Scaling Instead of Direct Multiplication

Instead of simply multiplying by a factor, normalize the probabilities across a wider range:

from sklearn.preprocessing import MinMaxScaler

# Stretch probabilities across a wider range (e.g., 0.001 to 0.02)
scaler = MinMaxScaler(feature_range=(0.001, 0.02))
scaled_calibrated_probabilities = scaler.fit_transform(calibrated_probabilities.reshape(-1, 1)).flatten()

# Rescale to maintain average PD at the target level
scaling_factor = target_ct_value / np.mean(scaled_calibrated_probabilities)
scaled_calibrated_probabilities *= scaling_factor

This ensures a wider spread while maintaining the desired target average.


---

3. Apply Log-Odds Transformation Before Scaling

If probabilities are too concentrated, using log-odds (logit transformation) can help before applying scaling:

from scipy.special import logit, expit

# Convert to log-odds space
log_odds = logit(np.clip(calibrated_probabilities, 1e-6, 1-1e-6))

# Scale log-odds to increase variation
log_odds_scaled = log_odds * 1.2  # Adjust multiplier based on spread

# Convert back to probability space
scaled_calibrated_probabilities = expit(log_odds_scaled)

# Rescale to maintain target CT
scaling_factor = target_ct_value / np.mean(scaled_calibrated_probabilities)
scaled_calibrated_probabilities *= scaling_factor

This method helps if your probabilities are too tightly packed around a small range.


---

4. Add Controlled Noise to Break Concentration

If you still find too few unique probability buckets, adding small controlled noise can help:

noise = np.random.normal(0, 0.002, size=calibrated_probabilities.shape)
scaled_calibrated_probabilities = calibrated_probabilities * scaling_factor + noise

# Ensure probabilities remain within valid bounds
scaled_calibrated_probabilities = np.clip(scaled_calibrated_probabilities, 0, 1)

This keeps the overall distribution intact but ensures finer granularity.


---

Conclusion

If your probabilities are too compressed, power transformation or log-odds scaling helps.

If they are concentrated within a narrow range, min-max scaling can help.

If the final probabilities are still too discrete, adding small noise breaks ties.


You can experiment with these approaches and visualize the distributions before and after applying them. Let me know if you need help tuning these transformations!
