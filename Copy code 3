install.packages(c("car", "caret", "DescTools", "Kendall"))



cor(model_data_scaled[, scale_vars])


library(car)

# Calculate VIF from linear model (no intercept)
vif_model <- lm(as.matrix(model_data_scaled[, scale_vars]) ~ 1)
vif_values <- vif(vif_model)

vif_df <- data.frame(
  feature = names(vif_values),
  VIF = vif_values
)

print(vif_df)


library(caret)     # confusionMatrix
library(DescTools) # KendallTauB
library(dplyr)

# 1. Predicted class probabilities
predicted_probs <- predict(model, type = "prob")

# 2. Predicted categories = class with highest probability
predicted_class <- colnames(predicted_probs)[apply(predicted_probs, 1, which.max)]

# 3. Convert to ordered factor (to match y)
predicted_factor <- factor(predicted_class, levels = levels(model_data_scaled$Economy_State_dummy), ordered = TRUE)
true_factor <- model_data_scaled$Economy_State_dummy

# Accuracy ratio
accuracy <- mean(predicted_factor == true_factor)
cat(sprintf("Accuracy ratio (overall accuracy): %.4f\n", accuracy))


cm <- confusionMatrix(predicted_factor, true_factor)
cat("\nConfusion Matrix:\n")
print(cm$table)



distances <- abs(as.numeric(predicted_factor) - as.numeric(true_factor))
weighted_accuracy <- 1 - mean(distances / (length(levels(true_factor)) - 1))
cat(sprintf("\nWeighted accuracy (penalizes distant misclassifications): %.4f\n", weighted_accuracy))


tau_b <- KendallTauB(as.numeric(predicted_factor), as.numeric(true_factor))
cat(sprintf("\nKendall's Tau-b: %.4f\n", tau_b))


# Extract per-class metrics
precision <- cm$byClass[, "Precision"]
recall <- cm$byClass[, "Recall"]
f1 <- cm$byClass[, "F1"]

cat("\nPrecision per class:\n")
print(round(precision, 4))

cat("Recall per class:\n")
print(round(recall, 4))

cat("F1 Score per class:\n")
print(round(f1, 4))

# Macro-averaged metrics
cat(sprintf("\nMacro Precision: %.4f\n", mean(precision, na.rm = TRUE)))
cat(sprintf("Macro Recall:    %.4f\n", mean(recall, na.rm = TRUE)))
cat(sprintf("Macro F1 Score:  %.4f\n", mean(f1, na.rm = TRUE)))

# Weighted averages
support <- as.numeric(table(true_factor))
total <- sum(support)

precision_weighted <- sum(precision * support / total, na.rm = TRUE)
recall_weighted <- sum(recall * support / total, na.rm = TRUE)
f1_weighted <- sum(f1 * support / total, na.rm = TRUE)

cat(sprintf("\nWeighted Precision: %.4f\n", precision_weighted))
cat(sprintf("Weighted Recall:    %.4f\n", recall_weighted))
cat(sprintf("Weighted F1 Score:  %.4f\n", f1_weighted))


# Expected scores = sum of (category index * probability)
category_indices <- 1:length(levels(true_factor))
expected_scores <- as.numeric(predicted_probs %*% category_indices)

# Kendall tau approximation of concordance
tau <- KendallTauB(expected_scores, as.numeric(true_factor))
somers_d <- 2 * tau - 1
cat(sprintf("\nSomers' D: %.4f (range: -1 to 1, higher is better)\n", somers_d))


# Output predicted categories
Pred_cat <- data.frame(Predicted = predicted_factor)
print(Pred_cat)

# Output predicted probabilities (first few rows)
head(predicted_probs)
